# -*- coding: utf-8 -*-
"""finalprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10rNxC0H-fgTVJGkYogu47kUNHlXvPPQX
"""

import pandas as pd

# Load the dataset
data = pd.read_csv('/content/loan_approval_dataset.csv')  # Adjust the file path accordingly

# Check for missing values
missing_values = data.isnull().sum()
print('Missing Values:')
print(missing_values)

# Impute missing numerical values with mean
numerical_cols = data.select_dtypes(include='number').columns
data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())

# Impute missing categorical values with mode
categorical_cols = data.select_dtypes(include='object').columns
data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

# Verify that missing values are handled
updated_missing_values = data.isnull().sum()
print('\nUpdated Missing Values:')
print(updated_missing_values)

data.info()

# One-hot encoding for Education and Self_employed
data = pd.get_dummies(data, columns=['Education', 'Self_employed'], drop_first=True)


# Convert target variable to numerical
data['Loan_status'] = data['Loan_status'].apply(lambda x: 1 if x == 'Approved' else 0)

# Verify the encoding
print('Encoded Data:')
print(data.head())

from sklearn.preprocessing import MinMaxScaler, StandardScaler


# List of numerical features to be scaled
numerical_features = ['No_of_dependents', 'Income_annum', 'Loan_amount', 'Loan_term',
                      'Cibil_score', 'Residential_assets_value', 'Commercial_assets_value',
                      'Luxury_assets_value', 'Bank_asset_value']

# Min-Max scaling
min_max_scaler = MinMaxScaler()
data[numerical_features] = min_max_scaler.fit_transform(data[numerical_features])

# Standardization (z-score normalization)
standard_scaler = StandardScaler()
data[numerical_features] = standard_scaler.fit_transform(data[numerical_features])

# Verify the scaled data
print('Scaled Data:')
print(data.head())

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Generate a toy dataset
X, y = make_classification(n_samples=2000, n_features=10, random_state=42)

# Introduce noisy features
X_noisy = np.random.random((X.shape[0], 5))  # Adding 5 noisy features
X_with_noise = np.hstack((X, X_noisy))

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_with_noise, y, test_size=0.2, random_state=42)

# Train a LightGBM model
clf = lgb.LGBMClassifier(random_state=42)
clf.fit(X_train, y_train)
# Predict and calculate accuracy
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy*100)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Calculate Sensitivity, Specificity, Precision, F1-score
def calculate_metrics(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)
    precision = precision_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return sensitivity*100, specificity*100, precision*100, f1*100

# Predict using the LightGBM model
y_pred = clf.predict(X_test)

# Calculate metrics for LightGBM
sensitivity, specificity, precision, f1 = calculate_metrics(y_test, y_pred)
print("Metrics for LightGBM:")
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)
print("Precision:", precision)
print("F1 Score:", f1)

from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
import pandas as pd
import numpy as np
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Generate a toy dataset
X, y = make_classification(n_samples=2100, n_features=10, random_state=42)

# Introduce noisy features
X_noisy = np.random.random((X.shape[0], 5))  # Adding 5 noisy features
X_with_noise = np.hstack((X, X_noisy))

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_with_noise, y, test_size=0.2, random_state=42)

# Train AdaBoostClassifier
ada_classifier = AdaBoostClassifier(random_state=42)
ada_classifier.fit(X_train, y_train)

# Predict on the test set
ada_predictions = ada_classifier.predict(X_test)


# Train XGBoostClassifier
xgb_classifier = XGBClassifier(random_state=42)
xgb_classifier.fit(X_train, y_train)

# Predict on the test set
xgb_predictions = xgb_classifier.predict(X_test)

# Create an ensemble prediction by combining predictions
ensemble_predictions = np.round((xgb_predictions + ada_predictions) / 2)

# Evaluate ensemble accuracy
ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
print("Ensemble Accuracy:", ensemble_accuracy*100)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Calculate Sensitivity, Specificity, Precision, F1-score
def calculate_metrics(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)
    precision = precision_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return sensitivity*100, specificity*100, precision*100, f1*100

# Calculate metrics for Ensemble
ensemble_sensitivity, ensemble_specificity, ensemble_precision, ensemble_f1 = calculate_metrics(y_test, ensemble_predictions)
print("\nMetrics for Ensemble:")
print("Sensitivity:", ensemble_sensitivity)
print("Specificity:", ensemble_specificity)
print("Precision:", ensemble_precision)
print("F1 Score:", ensemble_f1)

import matplotlib.pyplot as plt

# Metrics data
models = ['LightGBM', 'Ensemble']
sensitivity_scores = [ sensitivity,ensemble_sensitivity]
specificity_scores = [ specificity,ensemble_specificity]
precision_scores = [ precision,ensemble_precision]
f1_scores = [ f1,ensemble_f1]



# Plotting the bar graph
x = np.arange(len(models))  # the label locations
width = 0.15  # the width of the bars

fig, ax = plt.subplots(figsize=(10, 6))

rects1 = ax.bar(x - 1.5*width, sensitivity_scores, width, label='Sensitivity')
rects2 = ax.bar(x - 0.5*width, specificity_scores, width, label='Specificity')
rects3 = ax.bar(x + 0.5*width, precision_scores, width, label='Precision')
rects4 = ax.bar(x + 1.5*width, f1_scores, width, label='F1 Score')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_xlabel('Models')
ax.set_ylabel('Scores (%)')  # Adjusted y-axis label
ax.set_title('Metrics Comparison by Model')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

# Add the actual scores on top of the bars
for rects in [rects1, rects2, rects3, rects4]:
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}%',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

fig.tight_layout()

plt.show()

import matplotlib.pyplot as plt

y = ['LightGBM', 'Ensemble']
x = [accuracy * 100, ensemble_accuracy * 100]

bar_width = 0.2

fig, ax = plt.subplots()
bars = ax.bar(y, x, width=bar_width, color='blue')

plt.ylabel('Accuracy (%)')
plt.xlabel('Model')
plt.title('Difference between Classification Model')

# Add percentage labels on the bars
for bar in bars:
    height = bar.get_height()
    ax.annotate(f'{height:.2f}%',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 2),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom')

plt.show()

import matplotlib.pyplot as plt

y = ['LightGBM', 'Ensemble']
x = [accuracy * 100, ensemble_accuracy * 100]

# Define the bar positions for each group
bar_width = 0.45  # Adjust this value to set the distance between bars

# Adjust the x-axis positions for each bar group
x_positions = range(len(y))

fig, ax = plt.subplots()
bars = ax.bar(x_positions, x, width=bar_width, color='blue')

# Set the x-axis labels and ticks
ax.set_xticks(x_positions)
ax.set_xticklabels(y)

plt.ylabel('Accuracy (%)')
plt.xlabel('Model')
plt.title('Difference between Classification Model')

# Add percentage labels on the bars
for bar in bars:
    height = bar.get_height()
    ax.annotate(f'{height:.2f}%',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 2),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom')

plt.show()